import torch
import torch.nn as nn
from torch.nn import functional as F
import math
import requests
import os
import matplotlib.pyplot as plt
import time

# --- 1. è¶…å‚æ•°ä¸å®éªŒè®¾ç½® ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
block_size = 64
batch_size = 32
max_iters = 2000
eval_interval = 200
learning_rate = 3e-4
weight_decay = 0.1
n_embd = 128
n_head = 4
n_layer = 4
dropout = 0.1

# MoE Bias ä¸“ç”¨å‚æ•°
num_experts = 8      # expertæ•°é‡ï¼ˆbiaså‘é‡æ•°é‡ï¼‰
top_k = 2           # æ¯æ¬¡é€‰æ‹©çš„top-k experts
router_hidden = 64  # routeréšè—å±‚ç»´åº¦

# --- 2. æ•°æ®åŠ è½½ ---
def get_shakespeare_data():
    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
    data_path = 'input.txt'
    if not os.path.exists(data_path):
        print("Downloading tiny shakespeare dataset...")
        with open(data_path, 'w') as f:
            f.write(requests.get(url).text)

    with open(data_path, 'r', encoding='utf-8') as f:
        text = f.read()

    chars = sorted(list(set(text)))
    vocab_size = len(chars)
    stoi = {ch: i for i, ch in enumerate(chars)}
    itos = {i: ch for i, ch in enumerate(chars)}
    encode = lambda s: [stoi[c] for c in s]
    decode = lambda l: ''.join([itos[i] for i in l])

    data = torch.tensor(encode(text), dtype=torch.long)
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]
    return train_data, val_data, vocab_size

train_data, val_data, vocab_size = get_shakespeare_data()

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

# --- 3. MoE Bias æ ¸å¿ƒç»„ä»¶ ---

class MoEBiasRouter(nn.Module):
    """MoEè·¯ç”±å™¨ï¼šæ ¹æ®è¾“å…¥é€‰æ‹©bias experts"""
    def __init__(self, d_model, num_experts, router_hidden=None):
        super().__init__()
        self.num_experts = num_experts
        
        if router_hidden is None:
            router_hidden = d_model // 2
            
        # ç®€å•çš„routerç½‘ç»œ
        self.router = nn.Sequential(
            nn.Linear(d_model, router_hidden),
            nn.ReLU(),
            nn.Linear(router_hidden, num_experts)
        )
        
        # åˆå§‹åŒ–routeræƒé‡è¾ƒå°ï¼Œé¿å…å¼€å§‹æ—¶è¿‡äºæ¿€è¿›
        for layer in self.router:
            if isinstance(layer, nn.Linear):
                nn.init.normal_(layer.weight, std=0.02)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        # å¯¹åºåˆ—ç»´åº¦å–å¹³å‡ä½œä¸ºroutingçš„ä¾æ®
        routing_input = x.mean(dim=1)  # (batch_size, d_model)
        logits = self.router(routing_input)  # (batch_size, num_experts)
        return logits

class MoEBiasExperts(nn.Module):
    """MoE Biasä¸“å®¶ä»¬ï¼šä¸€ç»„å¯å­¦ä¹ çš„biaså‘é‡"""
    def __init__(self, d_model, num_experts):
        super().__init__()
        self.num_experts = num_experts
        self.d_model = d_model
        
        # ä¸“å®¶biaså‘é‡ä»¬
        self.expert_biases = nn.Parameter(torch.zeros(num_experts, d_model))
        
        # åˆå§‹åŒ–ï¼šå°çš„éšæœºå€¼
        nn.init.normal_(self.expert_biases, std=0.02)
    
    def forward(self, expert_weights):
        # expert_weights: (batch_size, num_experts)
        # è¿”å›: (batch_size, d_model)
        return expert_weights @ self.expert_biases

class MoEBiasLayer(nn.Module):
    """å®Œæ•´çš„MoE Biaså±‚"""
    def __init__(self, d_model, num_experts=8, top_k=2, router_hidden=None):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.top_k = top_k
        
        self.router = MoEBiasRouter(d_model, num_experts, router_hidden)
        self.experts = MoEBiasExperts(d_model, num_experts)
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å› å­
        self.bias_scale = nn.Parameter(torch.ones(1) * 0.1)
        
        # ç”¨äºåˆ†æçš„ç»Ÿè®¡ä¿¡æ¯
        self.register_buffer('expert_usage', torch.zeros(num_experts))
        self.register_buffer('total_calls', torch.zeros(1))
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        batch_size, seq_len, d_model = x.shape
        
        # 1. è·å–routing logits
        routing_logits = self.router(x)  # (batch_size, num_experts)
        
        # 2. è®¡ç®—expertæƒé‡ï¼ˆä½¿ç”¨top-k + softmaxï¼‰
        if self.top_k < self.num_experts:
            # Top-ké€‰æ‹©
            top_k_values, top_k_indices = torch.topk(routing_logits, self.top_k, dim=-1)
            # åˆ›å»ºmask
            mask = torch.zeros_like(routing_logits).scatter_(-1, top_k_indices, 1.0)
            routing_logits = routing_logits * mask + (mask - 1) * 1e9  # maskæ‰étop-k
        
        expert_weights = F.softmax(routing_logits, dim=-1)  # (batch_size, num_experts)
        
        # 3. è·å–ç»„åˆçš„bias
        combined_bias = self.experts(expert_weights)  # (batch_size, d_model)
        
        # 4. å¹¿æ’­åˆ°sequence lengthå¹¶åº”ç”¨bias
        combined_bias = combined_bias.unsqueeze(1).expand(-1, seq_len, -1)  # (batch_size, seq_len, d_model)
        output = x + self.bias_scale * combined_bias
        
        # 5. æ›´æ–°ä½¿ç”¨ç»Ÿè®¡ï¼ˆç”¨äºåˆ†æï¼‰
        if self.training:
            with torch.no_grad():
                self.expert_usage += expert_weights.sum(dim=0)
                self.total_calls += batch_size
        
        return output
    
    def get_expert_usage_stats(self):
        """è¿”å›ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡"""
        if self.total_calls > 0:
            usage_percentage = (self.expert_usage / self.total_calls * 100).cpu().numpy()
            return usage_percentage
        return None

# --- 4. æ ‡å‡†Transformerç»„ä»¶ ---

class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        wei = q @ k.transpose(-2, -1) * C**-0.5
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        v = self.value(x)
        out = wei @ v
        return out

class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedForward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )
    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    def __init__(self, n_embd, n_head, use_moe_bias=False):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        
        # MoE Biaså±‚
        self.use_moe_bias = use_moe_bias
        if use_moe_bias:
            self.moe_bias = MoEBiasLayer(
                d_model=n_embd,
                num_experts=num_experts,
                top_k=top_k,
                router_hidden=router_hidden
            )

    def forward(self, x):
        # æ ‡å‡†transformer block
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        
        # å¯é€‰çš„MoE Biasè°ƒåˆ¶
        if self.use_moe_bias:
            x = self.moe_bias(x)
        
        return x

class MiniTransformer(nn.Module):
    def __init__(self, use_moe_bias=False):
        super().__init__()
        self.use_moe_bias = use_moe_bias
        
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, use_moe_bias=use_moe_bias) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))
        x = tok_emb + pos_emb
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        loss = None
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        return logits, loss

# --- 5. è®­ç»ƒä¸è¯„æµ‹ ---

@torch.no_grad()
def estimate_loss(model, model_name):
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_interval//4)
        for k in range(eval_interval//4):
            X, Y = get_batch(split)
            _, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out['val']

def analyze_moe_bias_usage(model, model_name):
    """åˆ†æMoE Biasä¸“å®¶ä½¿ç”¨æƒ…å†µ"""
    if not hasattr(model, 'use_moe_bias') or not model.use_moe_bias:
        return
        
    print(f"\nğŸ¯ {model_name} MoE Biasä½¿ç”¨åˆ†æ:")
    
    for i, block in enumerate(model.blocks):
        if hasattr(block, 'moe_bias'):
            usage_stats = block.moe_bias.get_expert_usage_stats()
            bias_scale = block.moe_bias.bias_scale.item()
            
            if usage_stats is not None:
                print(f"ç¬¬{i}å±‚:")
                print(f"  ä¸“å®¶ä½¿ç”¨ç‡: {usage_stats}")
                print(f"  å¹³å‡ä½¿ç”¨ç‡: {usage_stats.mean():.1f}%")
                print(f"  ä½¿ç”¨ç‡æ ‡å‡†å·®: {usage_stats.std():.1f}%")
                print(f"  åç½®ç¼©æ”¾å› å­: {bias_scale:.4f}")
                
                # åˆ†æè´Ÿè½½å‡è¡¡
                max_usage = usage_stats.max()
                min_usage = usage_stats.min()
                balance_score = 1.0 - (max_usage - min_usage) / 100.0
                print(f"  è´Ÿè½½å‡è¡¡åˆ†æ•°: {balance_score:.3f} (1.0=å®Œç¾å‡è¡¡)")

def run_experiment(model_class, model_name, **model_kwargs):
    print(f"\n--- ğŸš€ å¼€å§‹è®­ç»ƒ: {model_name} ---")
    model = model_class(**model_kwargs).to(device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # ç»Ÿè®¡MoE Biasç›¸å…³å‚æ•°
    if 'moe' in model_name.lower():
        moe_params = sum(p.numel() for name, p in model.named_parameters() 
                        if 'moe_bias' in name or 'expert' in name or 'router' in name)
        print(f"MoE Biaså‚æ•°é‡: {moe_params:,} ({moe_params/total_params*100:.1f}%)")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    losses = []
    best_val_loss = float('inf')
    start_time = time.time()
    
    # åˆå§‹è¯„ä¼°
    initial_val_loss = estimate_loss(model, model_name)
    losses.append(initial_val_loss.item())
    print(f"åˆå§‹éªŒè¯æŸå¤±: {initial_val_loss:.4f}")
    
    for i in range(max_iters):
        xb, yb = get_batch('train')
        _, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        
        if (i + 1) % eval_interval == 0 or i == max_iters - 1:
            val_loss = estimate_loss(model, model_name)
            losses.append(val_loss.item())
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
            
            print(f"Step {i+1}: train loss {loss:.4f}, val loss {val_loss:.4f}, best val {best_val_loss:.4f}")

    end_time = time.time()
    print(f"--- âœ… {model_name} è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {end_time - start_time:.2f} ç§’ ---")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {losses[-1]:.4f}, æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    return losses, model

if __name__ == '__main__':
    # å®šä¹‰ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
    models_to_test = {
        "æ ‡å‡†Transformer": (MiniTransformer, {"use_moe_bias": False}),
        "MoE Bias Transformer": (MiniTransformer, {"use_moe_bias": True}),
    }
    
    results = {}
    trained_models = {}
    
    for name, (model_cls, kwargs) in models_to_test.items():
        losses, model = run_experiment(model_cls, name, **kwargs)
        results[name] = losses
        trained_models[name] = model
        
        # åˆ†æMoE Biasä½¿ç”¨æƒ…å†µ
        analyze_moe_bias_usage(model, name)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”
    plt.figure(figsize=(15, 5))
    
    # å­å›¾1: è®­ç»ƒæ›²çº¿
    plt.subplot(1, 3, 1)
    colors = ['blue', 'red']
    
    for i, (name, losses) in enumerate(results.items()):
        steps = list(range(0, len(losses) * eval_interval, eval_interval))
        if len(steps) > len(losses):
            steps = steps[:len(losses)]
        plt.plot(steps, losses, label=name, color=colors[i], marker='o', linewidth=2)
    
    plt.title("MoE Bias vs æ ‡å‡†Transformer")
    plt.xlabel("è®­ç»ƒæ­¥æ•°")
    plt.ylabel("éªŒè¯æŸå¤±")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒï¼ˆå¦‚æœæœ‰MoE Biasæ¨¡å‹ï¼‰
    if "MoE Bias Transformer" in trained_models:
        plt.subplot(1, 3, 2)
        moe_model = trained_models["MoE Bias Transformer"]
        
        # æ”¶é›†æ‰€æœ‰å±‚çš„ä¸“å®¶ä½¿ç”¨ç‡
        all_usage = []
        layer_labels = []
        
        for i, block in enumerate(moe_model.blocks):
            if hasattr(block, 'moe_bias'):
                usage_stats = block.moe_bias.get_expert_usage_stats()
                if usage_stats is not None:
                    all_usage.append(usage_stats)
                    layer_labels.append(f"Layer {i}")
        
        if all_usage:
            # ç»˜åˆ¶ä¸“å®¶ä½¿ç”¨ç‡çƒ­å›¾
            import numpy as np
            usage_matrix = np.array(all_usage)
            im = plt.imshow(usage_matrix, cmap='Blues', aspect='auto')
            plt.colorbar(im, label='ä½¿ç”¨ç‡ (%)')
            plt.xlabel('ä¸“å®¶ID')
            plt.ylabel('å±‚')
            plt.title('ä¸“å®¶ä½¿ç”¨ç‡åˆ†å¸ƒ')
            plt.yticks(range(len(layer_labels)), layer_labels)
    
    # å­å›¾3: å‚æ•°æ•ˆç‡å¯¹æ¯”
    plt.subplot(1, 3, 3)
    model_names = list(results.keys())
    final_losses = [results[name][-1] for name in model_names]
    param_counts = [sum(p.numel() for p in trained_models[name].parameters()) for name in model_names]
    
    # æ•£ç‚¹å›¾ï¼šå‚æ•°é‡ vs æ€§èƒ½
    colors_scatter = ['blue', 'red']
    for i, name in enumerate(model_names):
        plt.scatter(param_counts[i]/1e6, final_losses[i], 
                   color=colors_scatter[i], s=100, label=name, alpha=0.7)
        plt.annotate(name, (param_counts[i]/1e6, final_losses[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.xlabel('å‚æ•°é‡ (M)')
    plt.ylabel('æœ€ç»ˆéªŒè¯æŸå¤±')
    plt.title('å‚æ•°æ•ˆç‡å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("moe_bias_comparison.png", dpi=150, bbox_inches='tight')
    print("\nğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° 'moe_bias_comparison.png'")
    plt.show()
    
    # ç»“æœæ€»ç»“
    print(f"\nğŸ“ˆ å®éªŒç»“æœæ€»ç»“:")
    print(f"{'='*60}")
    
    baseline_loss = results["æ ‡å‡†Transformer"][-1]
    moe_bias_loss = results["MoE Bias Transformer"][-1]
    
    improvement = (baseline_loss - moe_bias_loss) / baseline_loss * 100
    
    standard_params = sum(p.numel() for p in trained_models["æ ‡å‡†Transformer"].parameters())
    moe_bias_params = sum(p.numel() for p in trained_models["MoE Bias Transformer"].parameters())
    param_overhead = (moe_bias_params - standard_params) / standard_params * 100
    
    print(f"æ ‡å‡†Transformer     : æŸå¤±={baseline_loss:.4f}, å‚æ•°={standard_params:,}")
    print(f"MoE Bias Transformer: æŸå¤±={moe_bias_loss:.4f}, å‚æ•°={moe_bias_params:,}")
    print(f"æ€§èƒ½æ”¹è¿›: {improvement:+.2f}%")
    print(f"å‚æ•°å¼€é”€: +{param_overhead:.1f}%")
    
    if improvement > 0:
        efficiency_ratio = improvement / param_overhead
        print(f"æ•ˆç‡æ¯” (æ”¹è¿›%/å‚æ•°å¼€é”€%): {efficiency_ratio:.2f}")
        print(f"ğŸ‰ MoE Biasæœ‰æ•ˆï¼ä»¥{param_overhead:.1f}%çš„å‚æ•°å¼€é”€è·å¾—äº†{improvement:.2f}%çš„æ€§èƒ½æå‡")
    else:
        print(f"ğŸ¤” MoE Biasåœ¨å½“å‰è®¾ç½®ä¸‹æ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°")
    
    print(f"\nğŸ’¡ MoE BiasæŠ€æœ¯æ€»ç»“:")
    print(f"âœ… æä½çš„å‚æ•°å¼€é”€ï¼ˆä¸»è¦æ˜¯biaså‘é‡ï¼‰")
    print(f"âœ… æä½çš„è®¡ç®—å¼€é”€ï¼ˆåªæ˜¯åŠ æ³•æ“ä½œï¼‰") 
    print(f"âœ… åŠ¨æ€çš„å†…å®¹ç›¸å…³è°ƒåˆ¶èƒ½åŠ›")
    print(f"âœ… å¯è§£é‡Šçš„ä¸“å®¶ä½¿ç”¨æ¨¡å¼")
